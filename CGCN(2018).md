# Graph Convolution over Pruned Dependency Trees Improves Relation Extraction
## Abstract 
依存树模型能够捕捉到词间的长距离关系，序列模型适合于短句子的关系捕捉。<br>
但是目前的依存树模型不能取得很好表现：<br>
（1）过度剪枝导致有用的信息被剪掉 <br>
（2）不同的树结构无法实现并行计算<br>
创新点：<br>
* 采用图卷积神经网络（GCN），实现了并行计算，抽取潜在信息<br>
* 提出一种新的剪枝算法为依存树瘦身<br>

`note:但是后面有人的实验展现出剪枝后的效果还不如未剪枝的好，这里值得思考一下`

## Architecture
![](https://github.com/tangshisong/NRE/blob/master/image/CGCN.png)<br>
 
该模型将句子以词向量输入网络，经过GCN层之后得到新的词向量，然后经过一个最大池化函数f得到三个输出向量，之后经过简单的拼接输入到FFNN得到最终的向量表示，最后将向量输入到一个线性softmax层得到概率值。<br>

现在来看一下具体的模型实现：<br>

input:句子的n个词向量![](https://latex.codecogs.com/gif.latex?h_%7B1%7D%5E%7B%280%29%7D%2C...%2Ch_%7Bn%7D%5E%7B%280%29%7D)
和句子的依存树的图表示![](https://latex.codecogs.com/gif.latex?A_%7Bn*n%7D)<br>

`note：两个节点之间有边记1否则即为0，此外模型以无向图为输入的，后续作者证明有向图（无论是自底向上还是自顶向下）会降低模型性能。以及作者在依存树的邻接矩阵中添加了自环，使得低层信息传入高层。最开始我在思考这个模型的改进的时候，就想在这里加一个attention，但是之后才发现居然有人做了。。。太尴尬了`

output：为给定关系的概率<br>

首先词向量进入到GCN层，每个向量从第l-1层到l层的变换公式：
![](https://latex.codecogs.com/png.latex?h_%7Bi%7D%5E%7B%28l%29%7D%3D%20%5Csigma%20%28%5Cfrac%7B%7B%7D%5Csum_%7Bj%3D1%7D%5E%7Bn%7DA_%7Bij%7DW%5E%7B%28l%29%7Dh_%7Bj%7D%5E%7B%28l-1%29%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7DA_%7Bij%7D%7D%20&amp;plus;b%5E%7B%28l%29%7D%29)<br>

每层都有自己的超参数：线性变换W和偏置b；由于分词表示的差异程度很大，分母实现了标准化；最后输入到非线性函数中<br>

## Encoding relation
这一部分主要是阐述对向量进行编码，即对GCN的输出再加工。<br>
（1）定义最大池化函数f，将GCN输出的d\*n矩阵转换为1\*d的句子向量，实际上是在选取最有用的特征<br>
（2）然后用f分别对三个部分进行max-pooling，即GCN的输出矩阵（句子级表示）、输出矩阵中的subject实体、输出矩阵中object实体（实体级表示）<br>
（3）对上面经过max-pooling后的输出按顺序concatenate一下得到一个新的representation<br>
（4）将这个representation输入到FFNN中，最后得到新的representation<br>
（5）然后经过一个softmax的线性层输出对于给定关系集合的概率表示<br>

## Improvement
基于上述的GCN model作者提出了两个改进策略：<br>
* Contextualized GCN<br>
作者提出目前的model还没有包含词序（一般的model都会加个PE）和词消歧<br>
所以作者先将句子输入到BiLSTM，然后再将BiLSTM的output作为GCN的输入，即CGCN。相比GCN，CGCN性能进一步提升。<br>
`note：目前为止，打算是将BiLSTM换了，或者改一下，留个坑，暂定。。。`

* Path-centric Pruning<br>
这里主要是讨论dependency-tree的pruning，毕竟在一句很长的话中，我们不可能关注所有的token，那邻接矩阵得多大，而且还有很多token对于我们来说不是很有用，甚至没用，所以pruning显得格外重要。经验发现，大部分的crucial information都在以relation为根且包含两个entity的LCA的子树中。所以很多就以这个为标准进行pruning，但是有时候的pruning太aggressive了，导致重要信息丢失，比如不起眼的单词“not”。<br>
作者提出一个K-distance的pruning方法，K=0时将树修建为一条路，K=1（实验最好的效果）时保留所有直接连接在路径上的节点，K为无穷时保留整个LCA子树。<br>

`note：我觉得pruning可以修改修改，想用动态剪枝试一试，但是感觉实现难度有点大，暂时没有更深入的想法。。。暂定留坑`

## Summary
作者用GCN来实现NRE，使得网络获取到更多相关信息，并使用BiLSTM进一步提升了model，还探讨了以路径为中心的剪枝算法，创新度还是不错吧。<br>
然后对于一些我有想法的点都note了一下，关于pruning是否真的一点用没有呢？直接在GCN前面加个BiLSTM会不会太唐突..有没有更好的优化方法？慢慢再思考思考吧。<br>
### 这是我第一篇Git文，哈哈哈哈，后续还会继续写这方面的paper阅读，以及如果有时间再总结一下各类NN以及其变体。





